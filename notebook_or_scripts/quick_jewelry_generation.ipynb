{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Jewelry Generation Demo\n",
        "## Reproducing Optimal Configuration in <10 Minutes\n",
        "\n",
        "This notebook demonstrates the key improvements from the research:\n",
        "- **Compel prompt weighting** for better jewelry term adherence\n",
        "- **Optimal parameter selection** via human evaluation\n",
        "- **Consistent modern aesthetics** matching target brands\n",
        "\n",
        "**Expected runtime**: 5-8 minutes on CPU, 2-3 minutes on GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# %pip install torch diffusers transformers compel matplotlib pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "from compel import Compel\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏆 Optimal Configuration (from Human Evaluation)\n",
        "\n",
        "Based on systematic testing and human evaluation:\n",
        "- **Model**: SD 1.5 (best balance of quality and speed)\n",
        "- **Sampler**: Euler Ancestral (best quality/speed balance)\n",
        "- **Strategy**: medium_compel (1.2x weight on jewelry terms)\n",
        "- **CFG Scale**: 9.0 (optimal prompt adherence)\n",
        "- **Steps**: 20 (sufficient quality, good speed)\n",
        "\n",
        "**Key Finding**: AI metrics (CLIP, LAION) poorly predicted human preferences (38.4%, 51.4% accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_pipeline(device=device):\n",
        "    \"\"\"Setup the optimized SD 1.5 pipeline\"\"\"\n",
        "    print(\"🔧 Loading SD 1.5 pipeline...\")\n",
        "    \n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    \n",
        "    # Load SD 1.5 with optimal configuration\n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\" if device == \"cuda\" else None\n",
        "    )\n",
        "    \n",
        "    # Set optimal sampler (from human evaluation)\n",
        "    pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "        pipeline.scheduler.config\n",
        "    )\n",
        "    \n",
        "    pipeline.to(device)\n",
        "    \n",
        "    # Memory optimizations\n",
        "    if device == \"cpu\":\n",
        "        pipeline.enable_attention_slicing()\n",
        "    else:\n",
        "        pipeline.enable_memory_efficient_attention()\n",
        "    \n",
        "    # Setup Compel for prompt weighting\n",
        "    compel = Compel(\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Pipeline ready on {device}\")\n",
        "    return pipeline, compel\n",
        "\n",
        "# Load the pipeline\n",
        "pipeline, compel = setup_pipeline()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
