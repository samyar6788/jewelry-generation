{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Before/After Images for Arcade AI Challenge\n",
        "\n",
        "Creates all 16 required images (8 prompts √ó baseline + optimized)\n",
        "\n",
        "This notebook implements the complete optimization strategy:\n",
        "- **LoRA adapters** for specific jewelry categories\n",
        "- **Special tokens** (sks, phol) for enhanced grounding\n",
        "- **Native diffusers attention weighting** for jewelry terms\n",
        "- **Optimal parameters** from human evaluation research\n",
        "\n",
        "## üöÄ Google Colab Ready\n",
        "All functions are included directly in this notebook - no external file dependencies!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image as IPImage\n",
        "from PIL import Image\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your paths and parameters here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== CONFIGURABLE PATHS ======\n",
        "# Change these paths as needed for your environment\n",
        "\n",
        "# Output directory for generated images\n",
        "OUTPUT_DIR = \"before_after\"  # For Colab/local: \"before_after\" \n",
        "                             # For specific path: \"/content/drive/MyDrive/jewelry_images\"\n",
        "\n",
        "# LoRA adapter paths (change if you have different structure)\n",
        "LORA_BASE_PATH = \"lora_adapters\"  # For Colab: upload to \"/content/lora_adapters\"\n",
        "\n",
        "# ====== DERIVED PATHS ======\n",
        "LORA_ADAPTERS = {\n",
        "    \"channel_set\": f\"{LORA_BASE_PATH}/channel-set/checkpoint/pytorch_lora_weights.safetensors\",\n",
        "    \"threader\": f\"{LORA_BASE_PATH}/threader/checkpoint/pytorch_lora_weights.safetensors\", \n",
        "    \"huggie\": f\"{LORA_BASE_PATH}/huggie/checkpoint/pytorch_lora_weights.safetensors\"\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"‚úÖ LoRA base path: {LORA_BASE_PATH}\")\n",
        "print(f\"‚úÖ LoRA adapters: {len(LORA_ADAPTERS)} configured\")\n",
        "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image as IPImage\n",
        "from PIL import Image\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The 8 required prompts (verbatim from challenge)\n",
        "REQUIRED_PROMPTS = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav√©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "# Special tokens for enhanced grounding\n",
        "SPECIAL_TOKENS = {\n",
        "    \"channel_set\": \"sks\",\n",
        "    \"threader\": \"phol\"\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Prompts loaded: {len(REQUIRED_PROMPTS)} prompts\")\n",
        "print(f\"‚úÖ Special tokens: {list(SPECIAL_TOKENS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "All the core functions for LoRA loading and prompt enhancement:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_jewelry_category(prompt):\n",
        "    \"\"\"Detect jewelry category for LoRA selection\"\"\"\n",
        "    prompt_lower = prompt.lower()\n",
        "    \n",
        "    if \"channel-set\" in prompt_lower:\n",
        "        return \"channel_set\"\n",
        "    elif \"threader\" in prompt_lower:\n",
        "        return \"threader\" \n",
        "    elif \"huggie\" in prompt_lower:\n",
        "        return \"huggie\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def load_lora_adapter(pipeline, category):\n",
        "    \"\"\"Load appropriate LoRA adapter for the jewelry category\"\"\"\n",
        "    if category and category in LORA_ADAPTERS:\n",
        "        lora_path = LORA_ADAPTERS[category]\n",
        "        if os.path.exists(lora_path):\n",
        "            print(f\"üîß Loading {category} LoRA adapter...\")\n",
        "            pipeline.load_lora_weights(lora_path)\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  LoRA adapter not found: {lora_path}\")\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "def unload_lora_adapter(pipeline):\n",
        "    \"\"\"Unload current LoRA adapter\"\"\"\n",
        "    try:\n",
        "        pipeline.unload_lora_weights()\n",
        "    except:\n",
        "        # No LoRA to unload or not supported\n",
        "        pass\n",
        "\n",
        "def apply_jewelry_enhancement(prompt, category=None):\n",
        "    \"\"\"Apply custom attention weighting using native diffusers syntax\"\"\"\n",
        "    \n",
        "    enhanced_prompt = prompt\n",
        "    \n",
        "    # Add special tokens for trained categories first\n",
        "    if category and category in SPECIAL_TOKENS:\n",
        "        special_token = SPECIAL_TOKENS[category]\n",
        "        if category == \"channel_set\" and \"channel-set\" in enhanced_prompt:\n",
        "            enhanced_prompt = enhanced_prompt.replace(\"channel-set\", f\"{special_token} channel-set\")\n",
        "        elif category == \"threader\" and \"threader\" in enhanced_prompt:\n",
        "            enhanced_prompt = enhanced_prompt.replace(\"threader\", f\"{special_token} threader\")\n",
        "    \n",
        "    # Apply prompt-specific attention weighting using diffusers syntax\n",
        "    if \"channel-set diamond eternity band\" in prompt:\n",
        "        # Prompt 1: channel-set\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"sks channel-set\", \"(sks channel-set:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"diamond\", \"(diamond:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"hammered\", \"(hammered:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"gold\", \"(gold:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"product-only\", \"(product-only:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"white background\", \"(white background:1.2)\")\n",
        "        \n",
        "    elif \"14k rose-gold threader earrings\" in prompt:\n",
        "        # Prompt 2: threader\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"rose-gold\", \"(rose-gold:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"phol threader\", \"(phol threader:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"bezel-set\", \"(bezel-set:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"diamond\", \"(diamond:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"lifestyle\", \"(lifestyle:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"macro\", \"(macro:1.2)\")\n",
        "        \n",
        "    elif \"organic cluster ring with mixed-cut sapphires\" in prompt:\n",
        "        # Prompt 3: organic cluster\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"organic cluster\", \"(organic cluster:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"sapphires\", \"(sapphires:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"diamonds\", \"(diamonds:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"brushed\", \"(brushed:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"platinum\", \"(platinum:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"modern\", \"(modern:1.2)\")\n",
        "        \n",
        "    elif \"solid gold cuff bracelet with blue sapphire\" in prompt:\n",
        "        # Prompt 4: cuff bracelet\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"gold\", \"(gold:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"cuff bracelet\", \"(cuff bracelet:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"blue sapphire\", \"(blue sapphire:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"refined\", \"(refined:1.2)\")\n",
        "        \n",
        "    elif \"modern signet ring, oval face, engraved gothic\" in prompt:\n",
        "        # Prompt 5: signet ring\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"modern\", \"(modern:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"signet\", \"(signet:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"engraved gothic initial 'M'\", \"(engraved gothic initial 'M':1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"sterling\", \"(sterling:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"silver\", \"(silver:1.2)\")\n",
        "        \n",
        "    elif \"delicate gold huggie hoops\" in prompt:\n",
        "        # Prompt 6: huggie hoops\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"delicate\", \"(delicate:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"gold\", \"(gold:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"huggie hoops\", \"(huggie hoops:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"contemporary\", \"(contemporary:1.2)\")\n",
        "        \n",
        "    elif \"stack of three slim rings\" in prompt:\n",
        "        # Prompt 7: ring stack\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"stack of three\", \"(stack of three:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"gold\", \"(gold:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"platinum\", \"(platinum:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"pav√©\", \"(pav√©:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"editorial\", \"(editorial:1.2)\")\n",
        "        \n",
        "    elif \"bypass ring with stones\" in prompt:\n",
        "        # Prompt 8: bypass ring\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"bypass ring\", \"(bypass ring:1.2)\")\n",
        "        enhanced_prompt = enhanced_prompt.replace(\"refined\", \"(refined:1.2)\")\n",
        "    \n",
        "    return enhanced_prompt\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Setup and Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_pipeline(device=\"cuda\"):\n",
        "    \"\"\"Setup SD 1.5 pipeline with optimal configuration\"\"\"\n",
        "    print(\"üîß Loading SD 1.5 pipeline...\")\n",
        "    \n",
        "    # Use FP16 on GPU for speed\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    variant = \"fp16\" if device == \"cuda\" else None\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=dtype,\n",
        "        variant=variant\n",
        "    )\n",
        "    \n",
        "    # Set optimal sampler (Euler Ancestral from human evaluation)\n",
        "    pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "        pipeline.scheduler.config\n",
        "    )\n",
        "    \n",
        "    pipeline.to(device)\n",
        "    \n",
        "    # Memory optimizations\n",
        "    if device == \"cuda\":\n",
        "        pipeline.enable_memory_efficient_attention()\n",
        "    else:\n",
        "        pipeline.enable_attention_slicing()\n",
        "    \n",
        "    print(f\"‚úÖ Pipeline ready on {device}\")\n",
        "    return pipeline\n",
        "\n",
        "def generate_baseline_image(pipeline, prompt, seed=42):\n",
        "    \"\"\"Generate baseline image with default settings\"\"\"\n",
        "    \n",
        "    generator = torch.Generator(device=pipeline.device).manual_seed(seed)\n",
        "    \n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=20,\n",
        "        guidance_scale=7.5,  # Default CFG\n",
        "        generator=generator,\n",
        "        height=512,\n",
        "        width=512\n",
        "    ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "def generate_optimized_image(pipeline, prompt, seed=42):\n",
        "    \"\"\"Generate optimized image with research-backed settings and LoRA\"\"\"\n",
        "    \n",
        "    # Detect jewelry category for LoRA selection\n",
        "    category = detect_jewelry_category(prompt)\n",
        "    \n",
        "    # Load appropriate LoRA adapter\n",
        "    lora_loaded = False\n",
        "    if category:\n",
        "        lora_loaded = load_lora_adapter(pipeline, category)\n",
        "    \n",
        "    # Apply prompt enhancement with special tokens and attention weighting\n",
        "    enhanced_prompt = apply_jewelry_enhancement(prompt, category)\n",
        "    \n",
        "    generator = torch.Generator(device=pipeline.device).manual_seed(seed)\n",
        "    \n",
        "    image = pipeline(\n",
        "        prompt=enhanced_prompt,      # Use enhanced prompt with native diffusers syntax\n",
        "        num_inference_steps=20,      # Optimal from research\n",
        "        guidance_scale=9.0,          # Optimal CFG from human evaluation\n",
        "        generator=generator,\n",
        "        height=512,\n",
        "        width=512\n",
        "    ).images[0]\n",
        "    \n",
        "    # Unload LoRA adapter for next generation\n",
        "    if lora_loaded:\n",
        "        unload_lora_adapter(pipeline)\n",
        "    \n",
        "    return image, enhanced_prompt, category\n",
        "\n",
        "def generate_all_comparisons():\n",
        "    \"\"\"Generate all 16 required images using configurable OUTPUT_DIR\"\"\"\n",
        "    \n",
        "    # Setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üöÄ Starting generation on {device}\")\n",
        "    print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\\\\n\")\n",
        "    \n",
        "    pipeline = setup_pipeline(device)\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    results = []\n",
        "    \n",
        "    for i, prompt in enumerate(REQUIRED_PROMPTS, 1):\n",
        "        print(f\"--- Prompt {i:02d}/08 ---\")\n",
        "        print(f\"'{prompt[:60]}...'\")\n",
        "        \n",
        "        # Generate baseline\n",
        "        print(\"üî∏ Generating baseline...\")\n",
        "        start_time = time.time()\n",
        "        baseline_image = generate_baseline_image(pipeline, prompt, seed=42)\n",
        "        baseline_time = time.time() - start_time\n",
        "        \n",
        "        # Save baseline\n",
        "        baseline_path = f\"{OUTPUT_DIR}/prompt{i:02d}_baseline.png\"\n",
        "        baseline_image.save(baseline_path)\n",
        "        \n",
        "        # Generate optimized\n",
        "        print(\"üîπ Generating optimized...\")\n",
        "        start_time = time.time()\n",
        "        optimized_image, enhanced_prompt, category = generate_optimized_image(\n",
        "            pipeline, prompt, seed=42\n",
        "        )\n",
        "        optimized_time = time.time() - start_time\n",
        "        \n",
        "        # Save optimized\n",
        "        optimized_path = f\"{OUTPUT_DIR}/prompt{i:02d}_yours.png\"\n",
        "        optimized_image.save(optimized_path)\n",
        "        \n",
        "        results.append({\n",
        "            'prompt_num': i,\n",
        "            'original_prompt': prompt,\n",
        "            'enhanced_prompt': enhanced_prompt,\n",
        "            'category': category,\n",
        "            'lora_used': category is not None,\n",
        "            'baseline_time': baseline_time,\n",
        "            'optimized_time': optimized_time,\n",
        "            'baseline_path': baseline_path,\n",
        "            'optimized_path': optimized_path\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Saved: {baseline_path}\")\n",
        "        print(f\"‚úÖ Saved: {optimized_path}\")\n",
        "        print(f\"‚è±Ô∏è  Times: baseline {baseline_time:.1f}s, optimized {optimized_time:.1f}s\\\\n\")\n",
        "    \n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    # Summary\n",
        "    print(\"üéØ GENERATION COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "    print(f\"üì∏ Images generated: {len(results) * 2}\")\n",
        "    print(f\"‚è±Ô∏è  Total time: {total_time:.1f}s\")\n",
        "    print(f\"‚ö° Average per image: {total_time/(len(results)*2):.1f}s\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Pipeline and generation functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Optimized Prompts\n",
        "\n",
        "Let's see what the enhanced prompts look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview first 3 optimized prompts\n",
        "for i, prompt in enumerate(REQUIRED_PROMPTS[:3], 1):\n",
        "    category = detect_jewelry_category(prompt)\n",
        "    enhanced = apply_jewelry_enhancement(prompt, category)\n",
        "    \n",
        "    print(f\"=== PROMPT {i:02d} ===\")\n",
        "    print(f\"Category: {'‚úÖ LoRA: ' + category if category else '‚ùå No LoRA'}\")\n",
        "    print(f\"Original: {prompt}\")\n",
        "    print(f\"Enhanced: {enhanced}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate All 16 Images\n",
        "\n",
        "‚ö†Ô∏è **Warning**: This will take a while to complete (especially on CPU). Each image takes ~1-3 minutes.\n",
        "\n",
        "The images will be saved to your configured `OUTPUT_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all 16 images (8 baseline + 8 optimized)\n",
        "results = generate_all_comparisons()\n",
        "\n",
        "print(f\"\\nüéØ GENERATION COMPLETE!\")\n",
        "print(f\"‚úÖ All 16 deliverable images ready!\")\n",
        "print(f\"üìÅ Check: {OUTPUT_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Before/After Images for Arcade AI Challenge\n",
        "\n",
        "Creates all 16 required images (8 prompts √ó baseline + optimized)\n",
        "\n",
        "This notebook implements the complete optimization strategy:\n",
        "- **LoRA adapters** for specific jewelry categories\n",
        "- **Special tokens** (sks, phol) for enhanced grounding\n",
        "- **Native diffusers attention weighting** for jewelry terms\n",
        "- **Optimal parameters** from human evaluation research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image as IPImage\n",
        "from PIL import Image\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The 8 required prompts (verbatim from challenge)\n",
        "REQUIRED_PROMPTS = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav√©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "# LoRA adapter paths and configuration\n",
        "LORA_ADAPTERS = {\n",
        "    \"channel_set\": \"../lora_adapters/channel-set/checkpoint/pytorch_lora_weights.safetensors\",\n",
        "    \"threader\": \"../lora_adapters/threader/checkpoint/pytorch_lora_weights.safetensors\", \n",
        "    \"huggie\": \"../lora_adapters/huggie/checkpoint/pytorch_lora_weights.safetensors\"\n",
        "}\n",
        "\n",
        "# Special tokens for enhanced grounding\n",
        "SPECIAL_TOKENS = {\n",
        "    \"channel_set\": \"sks\",\n",
        "    \"threader\": \"phol\"\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Configuration loaded: {len(REQUIRED_PROMPTS)} prompts, {len(LORA_ADAPTERS)} LoRA adapters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Copy the helper functions from the Python script:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy all functions from generate_before_after.py\n",
        "exec(open('../notebook_or_scripts/generate_before_after.py').read().split('if __name__ == \"__main__\":')[0])\n",
        "\n",
        "print(\"‚úÖ All functions loaded from generate_before_after.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup pipeline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipeline = setup_pipeline(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Prompts\n",
        "\n",
        "First, let's see what the optimized prompts look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview all optimized prompts\n",
        "for i, prompt in enumerate(REQUIRED_PROMPTS[:3], 1):  # Show first 3 for preview\n",
        "    category = detect_jewelry_category(prompt)\n",
        "    enhanced = apply_jewelry_enhancement(prompt, category)\n",
        "    \n",
        "    print(f\"=== PROMPT {i:02d} ===\")\n",
        "    print(f\"Category: {'‚úÖ LoRA: ' + category if category else '‚ùå No LoRA'}\")\n",
        "    print(f\"Original: {prompt}\")\n",
        "    print(f\"Enhanced: {enhanced}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Single Image\n",
        "\n",
        "Let's test with one prompt to make sure everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with the first prompt\n",
        "test_prompt = REQUIRED_PROMPTS[0]\n",
        "print(f\"Testing with: {test_prompt}\")\n",
        "\n",
        "# Generate baseline and optimized\n",
        "print(\"\\nüî∏ Generating baseline...\")\n",
        "baseline_image = generate_baseline_image(pipeline, test_prompt, seed=42)\n",
        "\n",
        "print(\"üîπ Generating optimized...\")\n",
        "optimized_image, enhanced_prompt, category = generate_optimized_image(pipeline, test_prompt, seed=42)\n",
        "\n",
        "print(f\"\\n‚úÖ Test complete!\")\n",
        "print(f\"Enhanced prompt: {enhanced_prompt}\")\n",
        "print(f\"Category: {category}\")\n",
        "\n",
        "# Display images side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "ax1.imshow(baseline_image)\n",
        "ax1.set_title(\"Baseline (CFG=7.5)\")\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(optimized_image)\n",
        "ax2.set_title(f\"Optimized (CFG=9.0, LoRA={category})\")\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate All 16 Images\n",
        "\n",
        "‚ö†Ô∏è **Warning**: This will take a while to complete (especially on CPU). Each image takes ~1-3 minutes to generate.\n",
        "\n",
        "Run the complete generation using the function from the script:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all 16 images (8 baseline + 8 optimized)\n",
        "# This uses the generate_all_comparisons() function from the script\n",
        "\n",
        "results = generate_all_comparisons()\n",
        "\n",
        "print(\"\\nüéØ GENERATION COMPLETE!\")\n",
        "print(\"‚úÖ All 16 deliverable images ready!\")\n",
        "print(\"üìÅ Check: deliverables/before_after/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
